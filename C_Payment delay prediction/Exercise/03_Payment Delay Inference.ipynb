{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4bdad155-8328-4dc9-ab93-c84782d13739",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Payment Delay Inference\n",
    "\n",
    "This notebook is designed to guide you through the process of setting up and analyzing time series data. Various time series analysis techniques will be applied to the prepared data. This may include visualization, statistical analysis, and forecasting. Finally, the results of the analysis will be presented and interpreted, providing insights and actionable information based on the time series data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "41ec32cb-f417-4bde-b273-0d779d21eab3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Install and Load Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "95e79367-0ac9-4c9f-a1de-22de5b25c638",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%pip install xgboost\n",
    "%pip install shap\n",
    "%restart_python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "55f18716-0297-432f-b385-c13d9838ad92",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import mlflow\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import row_number, lit, col, dateadd, monotonically_increasing_id\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import DoubleType, StructField, StructType, MapType, StringType\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "import shap\n",
    "from tqdm.auto import tqdm\n",
    "import databricks.connect as db_connect\n",
    "import mlflow.tracking._model_registry.utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0acc865c-f580-47f8-a1d9-56348d200a18",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "mlflow.set_tracking_uri(\"databricks\")\n",
    "mlflow.set_registry_uri(\"databricks-uc\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ad1250dc-ce38-4a4e-b6c7-1f63c03d9c05",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Set Parameters\n",
    "Please replace the values `<CATALOG_NAME>` and `<SCHEMA_NAME>` with the specific values that match our use case and group. You can find the correct names by checking the **Unity Catalog** and look for the specific catalog and schema names: `uc_XXX`, `grpX`. Additionally, please replace the value `<TIME_SERIES_TABLE_NAME>` with the according name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ad5e97cd-f246-4b02-8e60-40400d8e3954",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "-- CREATE CATALOG IF NOT EXISTS <CATALOG_NAME>;\n",
    "SET CATALOG <CATALOG_NAME>;\n",
    "-- CREATE SCHEMA IF NOT EXISTS <SCHEMA_NAME>;\n",
    "USE SCHEMA <SCHEMA_NAME>;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a665386a-6a1b-43c9-9abe-faf8a6f8afed",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Load Data Model\n",
    "Replace the value `<TABLE_NAME>` with the according table name for our preprocessed data and again set a random value for `<SEED_PARAMETER>`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "57edb308-12bb-4f9b-a210-3a542b22c51b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "inference_data_model = (spark.read.table(\"<TABLE_NAME>\")\n",
    "                        .drop(\"CompanyCode\", \"AccountingDocument\", \"FiscalYear\", \"AccountingDocumentItem\", \"delay\", \"NetDueDate\", \"ClearingDate\")\n",
    "                        .sample(0.01, seed=<SEED_PARAMETER>)\n",
    "                        .toPandas())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5f1b5a6a-be5a-4e8b-a590-de655bc6c471",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "inference_data_model.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ae50e308-73b1-4ed7-ad8b-44cd5b969b45",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def infer_column_dtype(series):\n",
    "    # Try to convert to numeric\n",
    "    try:\n",
    "        pd.to_numeric(series.dropna())\n",
    "        return 'numeric'\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    # Try to convert to datetime\n",
    "    try:\n",
    "        pd.to_datetime(series.dropna(), errors='raise', infer_datetime_format=True)\n",
    "        return 'datetime'\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    # If all unique values are 'True' or 'False' like\n",
    "    lower_vals = set(str(v).strip().lower() for v in series.dropna().unique())\n",
    "    if lower_vals <= {'true', 'false', '1', '0'}:\n",
    "        return 'boolean'\n",
    "    \n",
    "    return 'string'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "57acb17c-2aa3-492e-b71b-71e5ee6c245e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "for col in tqdm(inference_data_model.columns):\n",
    "    inferred = infer_column_dtype(inference_data_model[col])\n",
    "    if inferred == 'numeric':\n",
    "        inference_data_model[col] = pd.to_numeric(inference_data_model[col], errors='coerce')\n",
    "    elif inferred == 'datetime':\n",
    "        inference_data_model[col] = pd.to_datetime(inference_data_model[col], errors='coerce')\n",
    "    elif inferred == 'boolean':\n",
    "        inference_data_model[col] = inference_data_model[col].astype('bool')\n",
    "    else:\n",
    "        inference_data_model[col] = inference_data_model[col].astype('category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b9d69566-3aea-4015-833e-77122b9b7f10",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Set the registry URI manually\n",
    "mlflow.tracking._model_registry.utils._get_registry_uri_from_spark_session = lambda: \"databricks-uc\"\n",
    "\n",
    "mlflow.login()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "906c9ec5-ca66-44ca-a466-c6ab970f7d5a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Load Model\n",
    "Load the model by adjusting the value `<MODEL_NAME>` to the name of our model.\n",
    "\n",
    "Hint: You can find the name by checking for \"models\" in the **Unity Catalog** in the according catalog and schema. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bfae81cb-b291-4ca4-a847-5d4d83d4782f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "model_delay = mlflow.xgboost.load_model(\"models:/<MODEL_NAME>@prod\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c25ef980-54c0-4546-865d-3946e85295e6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "explainer = shap.TreeExplainer(model_delay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e17de703-2876-446e-892c-89cea70565c1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Convert the __TIMESTAMP column to a supported type\n",
    "inference_data_model['__TIMESTAMP'] = inference_data_model['__TIMESTAMP'].astype('int64')\n",
    "\n",
    "prediction = model_delay.predict(inference_data_model, output_margin=True)\n",
    "dexplain = xgb.DMatrix(inference_data_model, enable_categorical=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fc1aabb5-c07c-4d9d-9753-01367eacf704",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "explanation = explainer(dexplain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1102be70-429d-4427-ac6e-4f9c320b0f54",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Extract SHAP values from the explanation object\n",
    "shap_values = explanation.values\n",
    "\n",
    "# Get the list of columns from the inference data model\n",
    "column_list = list(inference_data_model.columns)\n",
    "\n",
    "# Create a Spark DataFrame from the SHAP values with appropriate schema\n",
    "spark_shap_df = spark.createDataFrame(\n",
    "    shap_values, \n",
    "    schema=StructType([StructField(f\"SHAP_Values_{column}\", DoubleType()) for column in column_list]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3d59c895-e767-4c14-94b3-025a53245c83",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Calculate the minimum and maximum values for each column in the SHAP values DataFrame\n",
    "mins = spark_shap_df.agg(*[F.min(c).alias(c) for c in spark_shap_df.columns])\n",
    "maxs = spark_shap_df.agg(*[F.max(c).alias(c) for c in spark_shap_df.columns])\n",
    "\n",
    "# Transpose the min/max values DataFrame for easier access\n",
    "mins_transposed = mins.pandas_api().transpose()\n",
    "maxs_transposed = maxs.pandas_api().transpose()\n",
    "\n",
    "# Identify columns where the minimum/maximum value is zero\n",
    "min_column_set = set(mins_transposed.loc[(mins_transposed[0] == 0)].index.tolist())\n",
    "max_column_set = set(maxs_transposed.loc[(maxs_transposed[0] == 0)].index.tolist())\n",
    "\n",
    "# Find and drop columns where both the minimum and maximum values are zero\n",
    "zero_only = list(min_column_set.intersection(max_column_set))\n",
    "spark_shap_df = spark_shap_df.drop(*zero_only)\n",
    "\n",
    "# Extract the original column names from the remaining SHAP value columns\n",
    "selected_columns = [column.split(\"SHAP_Values_\")[-1] for column in spark_shap_df.columns]\n",
    "\n",
    "# Add a unique ID column to the SHAP values DataFrame\n",
    "spark_shap_df = spark_shap_df.withColumn(\"ID\", F.monotonically_increasing_id())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d2b34c87-a861-4331-b655-c3f7538a1018",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "column_comments = spark.sql(\"DESCRIBE TABLE prepared_accounting_document\").filter(F.col(\"col_name\").isin(selected_columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "947b750e-d74f-46cd-ac2d-b908c533b96a",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": null,
       "filterBlob": "{\"filterGroups\":[{\"enabled\":true,\"filterGroupId\":\"fg_a9b4c871\",\"op\":\"OR\",\"filters\":[{\"filterId\":\"f_fe567246\",\"enabled\":true,\"columnId\":\"col_name\",\"dataType\":\"string\",\"filterType\":\"oneof\"}],\"local\":false,\"updatedAt\":1748273757758}],\"syncTimestamp\":1748273757758}",
       "queryPlanFiltersBlob": "[]",
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(column_comments)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "60037e92-e856-4af6-a819-d3a093593fbc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Replace the value `<TABLE_NAME>` with the appropriate table name from our preprocessed data. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e2fa7509-c469-4920-8249-76fa1d998089",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dataset_primary_keys = spark.read.table(\"<TABLE_NAME>\").select(\"CompanyCode\", \"AccountingDocument\", \"FiscalYear\", \"AccountingDocumentItem\", \"NetDueDate\", *selected_columns).sample(0.01, seed=42)\n",
    "\n",
    "# Add a unique identifier column \"ID\" to the dataset\n",
    "dataset_primary_keys = dataset_primary_keys.withColumn(\"ID\", monotonically_increasing_id())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5a9ade0c-8f47-4baf-9e54-3ed3055412f1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Replace the value `<PRIMARY_KEYS>` with the variable that we defined before to display a preview of the primary keys."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9d4bae02-fcd0-481f-91a2-539d03268061",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(<PRIMARY_KEYS>)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0b6349c5-c12b-4b32-b23a-b30ac09d22ee",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Set the `<JOIN_TYPE>` such that we create an `inner` join for the joined data frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c2328b84-e4ca-47d7-86b2-78f3a0a1158f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "joined_spark_shap_df = spark_shap_df.join(\n",
    "    dataset_primary_keys, on=\"ID\", how=\"<JOIN_TYPE>\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "488f5628-4b56-40c8-9e90-a90aed8307a7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Next, select the following columns for the table by replacing the value `<COL_NAME>`:\n",
    "- ID\n",
    "- CompanyCode\n",
    "- AccountingDocument\n",
    "- FiscalYear\n",
    "- AccountingDocumentItem\n",
    "- NetDueDate\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "51e3328a-c0fa-4bd2-8814-1667b9a662ed",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark_shap_structure = (joined_spark_shap_df.select(\n",
    "                F.col(\"<COL_NAME>\"),\n",
    "                F.col(\"<COL_NAME>\"),\n",
    "                F.col(\"<COL_NAME>\"),\n",
    "                F.col(\"<COL_NAME>\"),\n",
    "                F.col(\"<COL_NAME>\"),\n",
    "                F.col(\"<COL_NAME>\"),\n",
    "                F.array([\n",
    "                   F.struct(F.lit(column).alias(\"column_name\"),\n",
    "                            F.col(f\"SHAP_Values_{column}\").alias(\"shap_value\"),\n",
    "                            F.lit(column_comments.filter(column_comments.col_name == column).select(\"comment\").collect()[0][0]).alias(\"column_description\"),\n",
    "                            F.col(column).cast(\"string\").alias(\"column_value\")\n",
    "                            )\n",
    "                             for column in selected_columns]).alias(\"shap_array\")\n",
    "                ))\n",
    "display(spark_shap_structure)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5fe93517-3ea3-4c03-b08d-d0730ef61d93",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark_prediction_df = spark.createDataFrame(prediction, schema=StructType([StructField(\"delay_prediction\", DoubleType())])).select(monotonically_increasing_id().alias(\"ID\"), F.round(\"delay_prediction\").alias(\"delay_prediction\"))\n",
    "display(spark_prediction_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e4b64630-4e7b-4148-8ff9-9a5a46981265",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark_shap_data = (spark_shap_structure\n",
    "                   .join(spark_prediction_df, on=\"ID\", how=\"inner\")\n",
    "                   .drop(\"ID\")\n",
    "                   .select(\"CompanyCode\", \"AccountingDocument\", \"FiscalYear\", \"AccountingDocumentItem\", \"shap_array\", \"delay_prediction\", \"NetDueDate\"))\n",
    "display(spark_shap_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a2c312bc-05fc-4642-91f4-3c7ebdcca515",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Creation of Cashflow Prediction table\n",
    "We create a prediction table that contains a constraint key consisting of the columns AccountingDocument, CompanyCode, FiscalYear, AccountingDocumentItem. As the constraint key is unique over the complete Databricks catalog, please replace the constant `<CONSTRAINT_NAME>` with an appropriate name for the constraint key."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "58491d32-a18d-4dcb-8189-5d560a2f11f6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "In the following code, set the parameters `<BOOL>` as follows to make sure we prepare the table correctly for sharing it back to BDC:\n",
    "- enableChangeDataFeed = true\n",
    "- enableDeletionVectors = false "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b7834e81-11bd-4965-90f6-d173b9b7ccf8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as sf\n",
    "from pyspark.sql.types import StringType, ArrayType\n",
    "\n",
    "df = spark_shap_data.withColumn(\"shap_array\", sf.col(\"shap_array\").cast(ArrayType(StringType())))\n",
    "\n",
    "df.write.format(\"delta\").\\\n",
    "    mode(\"overwrite\").\\\n",
    "    option(\"delta.enableChangeDataFeed\", \"<BOOL>\").\\\n",
    "    option(\"delta.enableDeletionVectors\", \"<BOOL>\").\\\n",
    "    saveAsTable(\"delay_prediction_dataset_shap\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1bf2545a-78f4-4899-b376-cd746c4b3c5d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "ALTER TABLE delay_prediction_dataset_shap SET TBLPROPERTIES ('delta.columnMapping.mode' = 'name');\n",
    "\n",
    "ALTER TABLE delay_prediction_dataset_shap ALTER COLUMN CompanyCode SET NOT NULL;\n",
    "ALTER TABLE delay_prediction_dataset_shap ALTER COLUMN AccountingDocument SET NOT NULL;\n",
    "ALTER TABLE delay_prediction_dataset_shap ALTER COLUMN FiscalYear SET NOT NULL;\n",
    "ALTER TABLE delay_prediction_dataset_shap ALTER COLUMN AccountingDocumentItem SET NOT NULL;\n",
    "ALTER TABLE delay_prediction_dataset_shap ADD CONSTRAINT <CONSTRAINT_NAME> PRIMARY KEY (CompanyCode, AccountingDocument, FiscalYear, AccountingDocumentItem);\n",
    "ALTER TABLE delay_prediction_dataset_shap DROP COLUMN shap_array;\n",
    "ALTER TABLE delay_prediction_dataset_shap SET TBLPROPERTIES (\n",
    "  delta.enableChangeDataFeed = <BOOL>,\n",
    "  delta.enableDeletionVectors = <BOOL>\n",
    ");"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": {
    "hardware": {
     "accelerator": null,
     "gpuPoolId": null,
     "memory": "HIGH"
    }
   },
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 4788921342253729,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "03_Payment Delay Inference",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
