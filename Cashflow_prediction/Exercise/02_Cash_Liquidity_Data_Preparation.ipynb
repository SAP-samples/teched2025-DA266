{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f052a3d7-4d8b-45ab-910e-506c42894ae2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Cash Liquidity Forecast\n",
    "For the Data Product Cash Flow we want to expand the data product by calculating for upcoming periods the cash flow. This notebook shows an example workflow for the enrichment of the CashFlow data product which is going to be exposed back to SAP Datasphere in Business Data Cloud (BDC).\n",
    "This involves in total the following steps for the overall prediction:\n",
    "1. Install and import packages\n",
    "2. Load data from data product `Cashflow` \n",
    "3. Prepare data for time series forecasting\n",
    "4. Persist prepared time series data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ae446cb3-b42e-408f-af2e-80b0acd70978",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 1. Install and import packages\n",
    "All necessary packages for this notebook are going to be outlined in the following notebook cell. In order to make sure that the results are reproducible, the following packages are going to be installed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a9b5badb-a6fc-4423-aee5-aa5b0b62ddbe",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%pip install databricks-feature-engineering\n",
    "%restart_python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "02a63784-12dc-419a-b715-1760aaf66da5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from databricks.feature_engineering import FeatureEngineeringClient, FeatureLookup\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import to_date, col, date_trunc, sum, explode, sequence, min, max, lit, expr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fc5839e7-2160-46d6-989b-1751f0e6f7c3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "In order to isolate the created data assets, we create a catalog within Databricks and a respective schema within the catalog. Please replace the values `<CATALOG_NAME>` and `<SCHEMA_NAME>` with the specific values that match our use case and group. You can find the correct names by checking the **Unity Catalog** and look for the specific catalog and schema names:`uc_XXX`, `grpX`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ba69595f-8535-4dcd-bf43-e61b68273856",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "-- CREATE CATALOG IF NOT EXISTS <CATALOG_NAME>;\n",
    "SET CATALOG <CATALOG_NAME>;\n",
    "CREATE SCHEMA IF NOT EXISTS <SCHEMA_NAME>;\n",
    "USE SCHEMA <SCHEMA_NAME>;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "51809028-88a2-4dd0-8dc9-44eedf5590bd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 2. Load data from data product `Cashflow` "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6befe50a-09a5-47ed-be95-593122c62755",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "From BDC we expose a [delta enabled local table](https://help.sap.com/docs/SAP_DATASPHERE/c8a54ee704e94e15926551293243fd1d/154bdffb35814d5481d1f6de143a6b9e.html?locale=en-US) over the delta share which provides us with a table containing multiple entries for the same primary key. The dataset contains the __OPERATION_TYPE column marking the transactional statement (Insert, Update, Delete) together with the __TIMESTAMP column marking when this change happened. As we want to use the Cashflow transactional statements, we transform our dataset in the following to provide the most recent entry per primary key. In case the most recent entry is a deletion, we filter this record out.\n",
    "\n",
    "Replace the value `<TABLE_PATH>` with full path to the data product table `cashflow` in the delta share. You'll find the correct name by checking the **Unity Catalog**. \n",
    "\n",
    "Hint: The path structure in databricks follows the logic `share.schema.table`. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e1229c5e-02bd-4cda-b2d4-7b335eacccb5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "![find_cashflow_dataproduct2.png](../../images/find_cashflow_dataproduct2.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f7c9937f-d035-4ad9-a925-a1af6289d24c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName(\"cash_flow_data_preparation\").getOrCreate()\n",
    "data = spark.read.table(\"<TABLE_PATH>\")\n",
    "data = data.alias(\"l\").\\\n",
    "    groupBy(col(\"CashFlowID\")).agg(max(col(\"__TIMESTAMP\")).alias(\"__TIMESTAMP\")).\\\n",
    "    join(\n",
    "        data.alias(\"r\"), col(\"l.CashFlowID\") == col(\"r.CashFlowID\"), \"left\"\n",
    "    ).\\\n",
    "    where(\"'__OPERATION_TYPE' != 'D'\").\\\n",
    "    select(\"r.*\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5a44a916-25ba-4eb8-bcf1-207c48bda79b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 3. Prepare data for time series forecasting\n",
    "For the data preparation of the Cash Flow data product for the time series forecast, we remodel the data by performing the following steps. The date column is going to be the posting date as the posting marks whether a Cash flow is booked or not. The forecast is performed on a monthly date sequence on the posting date. [See details under term definition for posting date](https://help.sap.com/glossary/?locale=en-US&term=posting%2520date):\n",
    "1. Replace empty strings with Null values\n",
    "2. Select necessary columns and filter out on the Posting date invalid dates and Null values\n",
    "3. Floor Posting date column to month and rename date and value column\n",
    "4. Group data on date column and sum up Cash Flow per month\n",
    "5. Generate continuous time series range between minimum date and maximum date present in data\n",
    "6. Join generated time sequence to time series data in order to provide continuous time series dataframe\n",
    "7. Fill Null values with 0 as at those days no cash flow was recorded\n",
    "8. Convert Spark dataframe to pandas dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d393b966-ca5a-4705-940e-42677fb0ecec",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Floor date and rename columns\n",
    "data = data.\\\n",
    "    withColumn(\"PostingDate\", date_trunc(\"month\", col(\"PostingDate\")).cast(\"date\")).\\\n",
    "    withColumnsRenamed({\"PostingDate\": \"ds\", \"Company_Code\": \"CompanyCode\", \"AmountInCompanyCodeCurrency\": \"y\"})\n",
    "# aggregate time series on date and sum cash flow into a dataframe time_series_data\n",
    "time_series_data = data.\\\n",
    "    select(\"ds\", \"CompanyCode\", \"y\").\\\n",
    "    groupBy(\"ds\", \"CompanyCode\").\\\n",
    "    agg(sum(\"y\").alias(\"y\")).\\\n",
    "    orderBy(\"ds\")\n",
    "# generate continous time series sequence\n",
    "date_sequence_data = time_series_data\\\n",
    "    .select(\n",
    "        explode(\n",
    "            expr(\"sequence(min(ds), max(ds), INTERVAL 1 MONTH)\")\n",
    "            ).alias(\"ds\"))\n",
    "date_company_combination = time_series_data.select(\"CompanyCode\").\\\n",
    "    distinct().\\\n",
    "    join(date_sequence_data, how=\"cross\")\n",
    "# join time series data together with time series sequence\n",
    "time_series_data = time_series_data.\\\n",
    "    join(date_company_combination, on=[\"ds\", \"CompanyCode\"], how=\"right\").\\\n",
    "    fillna(0, subset=[\"y\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ff161f0b-9c08-4a10-b2d1-88aa780e88b1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### 4. Persist prepared time series data\n",
    "In order to reuse the dataset for our Training as well our Prediction, we store the transformed dataset into the feature store Databricks. This provides the possibility to not repeat the same data preparation script for both the Training as well as the Prediction notebook. In the following code, replace the values `<PRIMARY_KEY1>` and `<PRIMARY_KEY2>` by the correct column names `CompanyCode` and `ds` that were defined and joined in the previous step. Please also add a description for the placeholder `<DESCRIPTION>`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f471f65d-2a1e-4aa5-8496-ee0ed39d5859",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "fe_client = FeatureEngineeringClient()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e895cec3-35ec-4703-aaee-4744a32b7b06",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "fe_client.create_table(\n",
    "    name=\"prepared_cash_flow_time_series\",\n",
    "    primary_keys=[\"<PRIMARY_KEY1>\", \"<PRIMARY_KEY2>\"],\n",
    "    schema=time_series_data.schema,\n",
    "    description=\"<DESCRIPTION>\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e951beed-0160-43f9-904d-6eebe4ec6db8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "When writing the delta table in the very last step, please provide the correct table name for the value `<DF_NAME>`. \n",
    "\n",
    "Hint: Please use the df variable name that we created in the step before, containing the necessary data for the time series. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "52ba2412-b7b0-4cb6-a346-94c3540be58a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "fe_client.write_table(\n",
    "    name=\"prepared_cash_flow_time_series\",\n",
    "    df= DF_NAME\n",
    "    mode=\"merge\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bc5667d5-6f33-436d-a9f9-f3242975c700",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "When executed successfully, you should be able to find the created table in the unity catalog under the corresponding schema of your user."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e142ee61-effb-4737-9026-f4e492dac696",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "![](../../images/prepared_cf_table.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1828783c-3b99-4516-b07c-cee49dabdb79",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "1"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 5222401131272483,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "02_Cash_Liquidity_Data_Preparation",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
