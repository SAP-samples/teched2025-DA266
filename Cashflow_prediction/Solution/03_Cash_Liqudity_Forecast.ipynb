{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6040cf01-05bf-4f7e-b428-4f97780c7d82",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Cash Liquidity Forecast\n",
    "In this notebook we will forecast the cash flow by using the trained model and applying it to the data product `Cash Flow`.\n",
    "\n",
    "This involves in total the following steps for the overall prediction:\n",
    "- Retrieve logged trained model for prediction from MLflow\n",
    "- Write prediction results to Delta Table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b8334ecc-1b0b-4c3f-b564-660558ca4e37",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Install packages\n",
    "All necessary packages for this notebook are going to be outlined in the following notebook cell. In order to make sure that the results are reproducible, the following packages are going to be installed:\n",
    "- Mlflow: Used for tracking, storing and retrieving of our model\n",
    "- AutoTS: Package allowing us to run different Time Series algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1404bb30-1720-4d8e-bf13-1b64a1686b04",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%pip install mlflow\n",
    "%pip install autots['additional']\n",
    "%restart_python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "968e06d0-59ae-4015-82d3-f1d42b10a5c6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1ec11107-2064-476e-8a64-767c2e1c11b2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import mlflow\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, date_trunc, sum, explode, expr\n",
    "from delta import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d1ef1863-15e8-4aca-9354-1cdb0d6f104c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Setup Spark Session and consume data product\n",
    "\n",
    "Please replace the values `<CATALOG_NAME>` and `<SCHEMA_NAME>` with the specific values that match our use case and group. You can find the correct names by checking the **Unity Catalog** and look for the specific catalog and schema names:`uc_XXX`, `grpX`. Additionally, please replace the value `<TIME_SERIES_TABLE_NAME>` with the according name. \n",
    "\n",
    "Please note: \n",
    "We adapted the code here to match our use case. Therefore, some of the lines are commented out and not needed. However, they can be useful for future applications. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3f6075a8-072a-4da4-968d-a7474592cbbd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "-- CREATE CATALOG IF NOT EXISTS <CATALOG_NAME>;\n",
    "SET CATALOG uc_cash_liquidity_forecast;\n",
    "CREATE SCHEMA IF NOT EXISTS grp01;\n",
    "USE SCHEMA grp01"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "59fbffc6-319f-49de-a007-fd980c21168d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Please replace the value `<PREPARED_TABLE_NAME>` with the prepared cashflow table name, that we have created in the previous exercise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e076d402-a963-4321-82c0-75e54fb02613",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "builder = SparkSession.builder.appName(\"cash_flow_forecasting\").getOrCreate()\n",
    "data = spark.read.table(\"prepared_cash_flow_time_series\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "05df601e-3e51-4ca7-9b10-a412ad62b8c6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Time Series Forecasting\n",
    "We retrieve the model from the run from which we stored on MLflow for the Time Series training. After we retrieve the model from MLflow, we submit the spark dataframe to the predict function and retrieve the prediction from the function. After that we save the prediction data as a Delta Table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f3e31453-ad65-4de2-8847-1b16405f747a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "time_series_data = data.toPandas().astype({\"ds\": \"datetime64[ns]\", \"y\": float})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "18fa8880-0b6d-4aa8-8916-1bb8ee476481",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "%md\n",
    "Please replace the value `<USERNAME>` with your user name / email, e.g. ac229588u01@sapexperienceacademy.com ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "72a917f7-effd-4286-8efb-5d9d60d69ca4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "notebook_path = dbutils.notebook.entry_point.getDbutils().notebook().getContext().notebookPath().get()\n",
    "mlflow.set_tracking_uri(\"databricks\")\n",
    "mlflow.set_experiment(\"/Users/ac229588u01@sapexperienceacademy.com/Time Series Forecasting\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9e0a4c23-6cbe-429a-99fe-a1258605a28b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Retrieve MLflow model\n",
    "In order to get the model that we logged from our training procedure, we search in our MLflow experiment the last successful run ID and provide it to the mlflow functions in order to retrieve the prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "65240681-6df8-432d-9b67-e670d424ad88",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "last_run = mlflow.search_runs(order_by=[\"start_time DESC\"])\n",
    "run_id = last_run[last_run[\"status\"] == \"FINISHED\"][\"run_id\"].iloc[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ef897abd-d08b-4ba1-ae3e-758e547ec950",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "logged_model = f'runs:/{run_id}/model'\n",
    "\n",
    "# Load model as a PyFuncModel\n",
    "loaded_model = mlflow.pyfunc.load_model(logged_model)\n",
    "prediction = loaded_model.predict(time_series_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dd4f2244-727d-4157-8735-981b42871375",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "prediction = spark.createDataFrame(prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4fdf9c98-824e-4c2a-8352-194a1c194d0f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Creation of Cashflow Prediction table\n",
    "We create a prediction table that contains a constraint key consisting of the `date` and `CompanyCode` column. \n",
    "As the constraint key is unique over the complete Databricks catalog, please replace the constraint `<CONSTRAINT_NAME>` with an appropriate name for the constraint key. Additionally, replace the variable `<PREDICTION_RESULT_TABLE_NAME>` with the value `cashflow_prediction`.\n",
    "\n",
    "To be able to install the data product to Datasphere later on, the following requirements need to be fulfilled:\n",
    "\n",
    "- **Primary keys** are defined \n",
    "- **DeletionVectors** = disabled\n",
    "- **ChangeDataFeed** = enabled \n",
    "\n",
    "The following code will take care of this and set `Primary Key` and adjust the settings for `DeletionVectors` and `ChangeDataFeed`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0a3d59a3-8f4e-4e68-a54a-17e4684402d1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "prediction.write.format(\"delta\").\\\n",
    "    mode(\"overwrite\").\\\n",
    "    option(\"delta.enableChangeDataFeed\", \"true\").\\\n",
    "    option(\"delta.enableDeletionVectors\", \"false\").\\\n",
    "    saveAsTable(\"cashflow_prediction\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4a7eecdc-b750-4d79-ad46-532721e7653a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "ALTER TABLE cashflow_prediction ALTER COLUMN `date` SET NOT NULL;\n",
    "ALTER TABLE cashflow_prediction ALTER COLUMN CompanyCode SET NOT NULL;\n",
    "ALTER TABLE cashflow_prediction ADD CONSTRAINT PK_DATE_COMP1 PRIMARY KEY (`date`, CompanyCode);\n",
    "ALTER TABLE cashflow_prediction SET TBLPROPERTIES(\n",
    "  delta.enableDeletionVectors = false,\n",
    "  delta.enableChangeDataFeed = true\n",
    ");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4f945d42-4d4b-47b4-998e-a00df4f11b54",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "%md\n",
    "After successful execution, please validate the table `cashflow_prediction` in the Unity Catalog in your created SCHEMA and preview the prediction data:\n",
    "- open the `Catalog Explorer` by right-clicking on the table\n",
    "- navigate to the tab `sample data` \n",
    "- select compute `Serverless Starter Warehouse` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3c49e08e-ceb2-41f6-9e6f-da0bd7841721",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 5370185177402486,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 2
   },
   "notebookName": "03_Cash_Liqudity_Forecast",
   "widgets": {}
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
