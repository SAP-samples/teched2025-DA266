{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0f06fd25-da3c-457a-9aa7-b29c5dc88d99",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Payment Delay Inference\n",
    "This notebook will guide you through the process of setting up the prediction for the payment delays. To understand the factors influencing each delay prediction, we will use SHAP values [(SHapley Additive exPlanations)](https://shap.readthedocs.io/en/latest/index.html). SHAP values (SHapley Additive exPlanations) are a method used in machine learning to explain the output of predictive models. They help you understand how much each feature contributes to a particular prediction.\n",
    "\n",
    "This are the main steps of this exercise:\n",
    "1. Install and Import Packages\n",
    "2. Predict the Payment Delays\n",
    "3. Derive key drivers for payment delay prediction\n",
    "4. Merge prediction data and SHAP values into a single result set\n",
    "5. Persist the result set into a table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5d701a77-5fd0-4e1d-98e3-50feb7195e9e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 1. Install and Load Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "106ce0dc-8618-4925-9f4e-19563dbfb3e1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "%pip install xgboost\n",
    "%pip install shap\n",
    "%restart_python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0c4c6c96-909e-4a94-99db-079cd1871772",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import mlflow\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import row_number, lit, col, dateadd, monotonically_increasing_id\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import DoubleType, StructField, StructType, MapType, StringType\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "import shap\n",
    "from tqdm.auto import tqdm\n",
    "import databricks.connect as db_connect\n",
    "import mlflow.tracking._model_registry.utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b4b12118-242b-4cb6-b81e-b41299052e17",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 2. Predict the Payment Delays"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "cc471e48-3f41-47a2-90f6-b5aa72fb8f2f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Set Parameters\n",
    "Please replace the values `<CATALOG_NAME>` and `<SCHEMA_NAME>` with the specific values that match our use case and group. You can find the correct names by checking the **Unity Catalog** and look for the specific catalog and schema names: `uc_XXX`, `grpX`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bddef0c0-1caf-429c-bf0d-1ef1a84f9482",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "-- CREATE CATALOG IF NOT EXISTS ;\n",
    "SET CATALOG uc_delayed_payments;\n",
    "CREATE SCHEMA IF NOT EXISTS grp01;\n",
    "USE SCHEMA grp01;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "930c7d0a-2c96-4923-966e-315c9652638f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Load data model\n",
    "Replace the value `<PREPARED_TABLE_NAME>` with the prepared table name. We will load a randomized sample dataset by providing the variable `<SEED_PARAMETER>` with a random value, e.g. `42`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b11fd837-9d7a-4286-96cc-4ee58d871ce0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "mlflow.set_tracking_uri(\"databricks\")\n",
    "mlflow.set_registry_uri(\"databricks-uc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "70e91f4e-1eef-4e2e-9ad9-87642a49a44b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "inference_data_model = (spark.read.table(\"prepared_accounting_document\")\n",
    "                        .drop(\"CompanyCode\", \"AccountingDocument\", \"FiscalYear\", \"AccountingDocumentItem\", \"delay\", \"NetDueDate\", \"ClearingDate\")\n",
    "                        .sample(0.01, seed=42)\n",
    "                        .toPandas())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e09e9b7a-6f96-46db-87b6-2c6ca480a97a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "inference_data_model.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cefea901-3821-419b-8ea4-6c00350cf8ea",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def infer_column_dtype(series):\n",
    "    # Try to convert to numeric\n",
    "    try:\n",
    "        pd.to_numeric(series.dropna())\n",
    "        return 'numeric'\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    # Try to convert to datetime\n",
    "    try:\n",
    "        pd.to_datetime(series.dropna(), errors='raise', infer_datetime_format=True)\n",
    "        return 'datetime'\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    # If all unique values are 'True' or 'False' like\n",
    "    lower_vals = set(str(v).strip().lower() for v in series.dropna().unique())\n",
    "    if lower_vals <= {'true', 'false', '1', '0'}:\n",
    "        return 'boolean'\n",
    "    \n",
    "    return 'string'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "90b88780-67c3-4768-83fe-bcc1fb7f2f46",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "for col in tqdm(inference_data_model.columns):\n",
    "    inferred = infer_column_dtype(inference_data_model[col])\n",
    "    if inferred == 'numeric':\n",
    "        inference_data_model[col] = pd.to_numeric(inference_data_model[col], errors='coerce')\n",
    "    elif inferred == 'datetime':\n",
    "        inference_data_model[col] = pd.to_datetime(inference_data_model[col], errors='coerce')\n",
    "    elif inferred == 'boolean':\n",
    "        inference_data_model[col] = inference_data_model[col].astype('bool')\n",
    "    else:\n",
    "        inference_data_model[col] = inference_data_model[col].astype('category')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "41f5f75f-873a-42db-b6dc-a1620291b52b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Convert the __TIMESTAMP column to a supported type\n",
    "inference_data_model['__TIMESTAMP'] = inference_data_model['__TIMESTAMP'].astype('int64')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c301a675-4c39-4d55-81b0-ed9d0a9a8825",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Load Trained Model\n",
    "Load the trained model by replacing the variable `<TRAINED_MODEL_NAME>` with the trained model name from previous exercise.\n",
    "Hint: You can also find the name by checking for `models` in the **Unity Catalog** in the corresponding catalog and schema. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "06f684ef-201e-4c0e-8860-784a3e86bd2d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Set the registry URI manually\n",
    "mlflow.tracking._model_registry.utils._get_registry_uri_from_spark_session = lambda: \"databricks-uc\"\n",
    "mlflow.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cf5d73c5-7e10-4848-8341-9d7cdc30a9d3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "model_delay = mlflow.xgboost.load_model(\"models:/delay_prediction@prod\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "47c53e54-fbf9-4bd6-9a47-cc9c3cf55249",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Run Prediction\n",
    "We then perform inference using the trained model on our prepared data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7a3e180b-17a8-492e-9e3b-c0bacda8d9f8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "prediction = model_delay.predict(inference_data_model, output_margin=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b6194fb7-9cac-4fb1-8317-94e9cf1c2d37",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 3. Derive key drivers for payment delay prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "33051af7-46a7-42eb-a008-e84880bf59b9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "To determine the key influecing factors on payment delays, we will use SHAP TreeExplainer and XGBoost (Extreme Gradient Boosting) algorithm.\n",
    "\n",
    "- [**SHAP TreeExplainer**](https://shap.readthedocs.io/en/latest/tabular_examples.html#tree-based-models) is a specialized explainer framework, designed specifically for tree-based machine learning models. It provides fast and exact explanations for how each feature contributes to a prediction.\n",
    "- [**XGBoost**]((https://xgboost.ai/about) is a powerful and efficient machine learning algorithm based on gradient boosting, to predict the payment delays. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "18ee22bb-2743-4a99-adbd-2f4b70749873",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "explainer = shap.TreeExplainer(model_delay)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3a1c5084-f41f-4e4e-94c6-289e8f543f9e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Converting data into xgb.DMatrix data structure, which is used in XGBoost to efficiently store and process input data for training and prediction. It’s optimized for performance and memory usage, especially with large datasets.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7c75985b-5e09-4107-b5e6-a5494f951d44",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "Xd = xgb.DMatrix(inference_data_model, enable_categorical=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5fe527e0-9e25-4a25-95d5-56fde4d07586",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "Xd.feature_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e6ddceef-8f47-44a0-8e0d-cdbd3d1853bd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "explanation = explainer(Xd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a73d2a47-5fa9-40a6-8f20-867b51f94822",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "explanation.feature_names = Xd.feature_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f494395b-743e-4b8a-be04-a6b4b85986de",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Visualizing the SHAP values using beeswarm plot\n",
    "\n",
    "The [beeswarm plot](https://shap.readthedocs.io/en/latest/example_notebooks/api_examples/plots/beeswarm.html#A-simple-beeswarm-summary-plot) is designed to display an information-dense summary of how the top features in a dataset impact the model’s output. Each instance the given explanation is represented by a single dot on each feature row. The x position of the dot is determined by the SHAP value (`shap_values.value[instance,feature]`) of that feature, and dots “pile up” along each feature row to show density. Color is used to display the original value of a feature (`shap_values.data[instance,feature]`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f4137b9a-a576-4eb6-9888-b373c3659dae",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "shap.plots.beeswarm(explanation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c629ca91-37a0-4f9c-9bf0-66f4e32bc08d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Sometimes it is helpful to transform the SHAP values before we plots them. Below we plot the absolute value and fix the color to be red. This creates a richer parallel to the standard shap_values.abs.mean(0) bar plot, since the bar plot just plots the mean value of the dots in the beeswarm plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b0422b63-6fec-416f-a874-0098539a469f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "shap.plots.bar(explanation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ac0a4560-5825-48c6-a4ea-9225498f3ddc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "%md\n",
    "## 4. Merge prediction data and SHAP values into a single result set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bc4ab6f1-638b-4107-836b-410544beb3f3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Extract SHAP values from the explanation object\n",
    "shap_values = explanation.values\n",
    "\n",
    "# Get the list of columns from the inference data model\n",
    "column_list = list(inference_data_model.columns)\n",
    "\n",
    "# Create a Spark DataFrame from the SHAP values with appropriate schema\n",
    "spark_shap_df = spark.createDataFrame(\n",
    "    shap_values, \n",
    "    schema=StructType([StructField(f\"SHAP_Values_{column}\", DoubleType()) for column in column_list]))\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "66750e95-ac2e-48cb-b89c-f203d92ac625",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark_shap_df.display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d7a4ad8e-eaca-41c1-a69b-ec9dda00b0d4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Calculate Min/Max Values for each column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6c3d5664-9000-4a96-93ed-d57e13126993",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Calculate the minimum and maximum values for each column in the SHAP values DataFrame\n",
    "mins = spark_shap_df.agg(*[F.min(c).alias(c) for c in spark_shap_df.columns])\n",
    "maxs = spark_shap_df.agg(*[F.max(c).alias(c) for c in spark_shap_df.columns])\n",
    "\n",
    "# Transpose the min/max values DataFrame for easier access\n",
    "mins_transposed = mins.pandas_api().transpose()\n",
    "maxs_transposed = maxs.pandas_api().transpose()\n",
    "\n",
    "# Identify columns where the minimum/maximum value is zero\n",
    "min_column_set = set(mins_transposed.loc[(mins_transposed[0] == 0)].index.tolist())\n",
    "max_column_set = set(maxs_transposed.loc[(maxs_transposed[0] == 0)].index.tolist())\n",
    "\n",
    "# Find and drop columns where both the minimum and maximum values are zero\n",
    "zero_only = list(min_column_set.intersection(max_column_set))\n",
    "spark_shap_df = spark_shap_df.drop(*zero_only)\n",
    "\n",
    "# Extract the original column names from the remaining SHAP value columns\n",
    "selected_columns = [column.split(\"SHAP_Values_\")[-1] for column in spark_shap_df.columns]\n",
    "\n",
    "# Add a unique ID column to the SHAP values DataFrame\n",
    "spark_shap_df = spark_shap_df.withColumn(\"ID\", F.monotonically_increasing_id())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "389ebcfa-c474-4f27-8c86-34d1f26db969",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Retrieve additional semantics\n",
    "In this step, we enrich the table meta data with additional business semantics, by providing addtional comments / descriptions per column."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1a0acaf2-d9ef-4a6b-aff5-07d14140afe1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "> [IMPORTANT]\n",
    "> This step is required as a preparation for later exercise, when we will interprete the result set using LLM! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b1cdee85-754b-45fb-aa34-7a130dd2315f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "1. Navigate to the Unity Catalog and find in your `<SCHEMA>` the prepared table `prepared_accounting_document`. \n",
    "2. Right-click on the table to open the `Catalog Explorer`\n",
    "3. Go the tab `Overview` to see the list of all columns\n",
    "4. Click on the `AI generate` button on the top right\n",
    "5. Confirm the dialog with the AI generated comments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "23b7bcea-232a-449b-bd8f-1e22725e5015",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "After successful generation of the column comments, proceed with the exercise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "86858e7d-a166-4279-ac14-084cf5399282",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "column_comments = spark.sql(\"DESCRIBE TABLE prepared_accounting_document\").filter(F.col(\"col_name\").isin(selected_columns))\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2bb1de40-53d6-4003-a074-72eadfcb2a53",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(column_comments)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "46bc113c-f0f5-4980-b6b0-bfc4b643ecd8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Replace the value \\<PREPARED_TABLE_NAME\\> with the appropriate table name from our preprocessed data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "20c214a1-91e8-4e6c-9291-3a91390cee6f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dataset_primary_keys = spark.read.table(\"prepared_accounting_document\").select(\"CompanyCode\", \"AccountingDocument\", \"FiscalYear\", \"AccountingDocumentItem\", \"NetDueDate\", *selected_columns).sample(0.01, seed=42)\n",
    "\n",
    "# Add a unique identifier column \"ID\" to the dataset\n",
    "dataset_primary_keys = dataset_primary_keys.withColumn(\"ID\", monotonically_increasing_id())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "12a1d04c-ca52-48db-bf69-914b60a95cfa",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Replace the value \\<PRIMARY_KEYS\\> with the variable that we defined before to display a preview of the primary keys."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0028f08c-5654-4ea2-8f9c-e357afb4618c",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1759930901857}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(dataset_primary_keys)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4df38931-618f-49ac-aa15-d49ac4b3b26c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Set the `<JOIN_TYPE>` to the value `inner`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "363cf123-0dfb-4854-b61c-70ced7dc527c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "joined_spark_shap_df = spark_shap_df.join(\n",
    "    dataset_primary_keys, on=\"ID\", how=\"inner\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8ec8292f-f72c-4a66-ad1a-914faec28342",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1759932848553}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "joined_spark_shap_df.display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "41fd0700-c113-4d74-9562-562494e3f25a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Next, select the following columns for the table by replacing the value \\<COL_NAME\\> and append the generated SHAP values into the structure of the table:\n",
    "\n",
    "- ID\n",
    "- CompanyCode\n",
    "- AccountingDocument\n",
    "- FiscalYear\n",
    "- AccountingDocumentItem\n",
    "- NetDueDate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d4d02191-3bf4-49ab-9e85-49bc46c0f1d6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark_shap_structure = (joined_spark_shap_df.select(\n",
    "                F.col(\"ID\"),\n",
    "                F.col(\"CompanyCode\"),\n",
    "                F.col(\"AccountingDocument\"),\n",
    "                F.col(\"FiscalYear\"),\n",
    "                F.col(\"AccountingDocumentItem\"),\n",
    "                F.col(\"NetDueDate\"),\n",
    "                F.array([\n",
    "                   F.struct(F.lit(column).alias(\"column_name\"),\n",
    "                            F.col(f\"SHAP_Values_{column}\").alias(\"shap_value\"),\n",
    "                            F.lit(column_comments.filter(column_comments.col_name == column).select(\"comment\").collect()[0][0]).alias(\"column_description\"),\n",
    "                            F.col(column).cast(\"string\").alias(\"column_value\")\n",
    "                            )\n",
    "                             for column in selected_columns]).alias(\"shap_array\")\n",
    "                ))\n",
    "display(spark_shap_structure)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "789f3ccd-d31a-4e7e-95b6-a59a91662689",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark_prediction_df = spark.createDataFrame(prediction, schema=StructType([StructField(\"delay_prediction\", DoubleType())])).select(monotonically_increasing_id().alias(\"ID\"), F.round(\"delay_prediction\").alias(\"delay_prediction\"))\n",
    "display(spark_prediction_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4e359333-fa7a-4019-b74b-1d772be56833",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark_shap_data = (spark_shap_structure\n",
    "                   .join(spark_prediction_df, on=\"ID\", how=\"inner\")\n",
    "                   .drop(\"ID\")\n",
    "                   .select(\"CompanyCode\", \"AccountingDocument\", \"FiscalYear\", \"AccountingDocumentItem\", \"shap_array\", \"delay_prediction\", \"NetDueDate\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b90bb8bd-27ac-40fb-abfa-ae183ba9e3bc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(spark_shap_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1a90eca1-e4ce-4686-8d7f-b0d5d10466ff",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 5. Persist the result set into a table\n",
    "We create a prediction table that contains a constraint key consisting of the columns AccountingDocument, CompanyCode, FiscalYear, AccountingDocumentItem. As the constraint key is unique over the complete Databricks catalog, please replace the constant `<CONSTRAINT_NAME>` with an appropriate name for the constraint key."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "833d3f2f-e3fc-44a6-93d4-7e8d80b55362",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "In the following code, set the parameters \\<BOOL\\> as follows to make sure we prepare the table correctly for sharing it back to BDC:\n",
    "\n",
    "- enableChangeDataFeed = true\n",
    "- enableDeletionVectors = false"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ffaa7364-2163-47fb-9c8a-689f3849532a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark_shap_data.write.format(\"delta\").\\\n",
    "    mode(\"overwrite\").\\\n",
    "    option(\"delta.enableChangeDataFeed\", \"true\").\\\n",
    "    option(\"delta.enableDeletionVectors\", \"false\").\\\n",
    "    option(\"mergeSchema\", \"true\").\\\n",
    "    saveAsTable(\"delay_prediction_shap\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2b98ba93-8acf-4941-b3c0-795b28ce8952",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "ALTER TABLE delay_prediction_shap SET TBLPROPERTIES ('delta.columnMapping.mode' = 'name');\n",
    "\n",
    "ALTER TABLE delay_prediction_shap ALTER COLUMN CompanyCode SET NOT NULL;\n",
    "ALTER TABLE delay_prediction_shap ALTER COLUMN AccountingDocument SET NOT NULL;\n",
    "ALTER TABLE delay_prediction_shap ALTER COLUMN FiscalYear SET NOT NULL;\n",
    "ALTER TABLE delay_prediction_shap ALTER COLUMN AccountingDocumentItem SET NOT NULL;\n",
    "ALTER TABLE delay_prediction_shap ADD CONSTRAINT delay_shap_pk2 PRIMARY KEY (CompanyCode, AccountingDocument, FiscalYear, AccountingDocumentItem);\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6427c2b3-cb03-433e-a5f4-e745c6ff5826",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "To be able to share later the table as data product, the following requirements should be fulfilled:\n",
    "- table has primary keys \n",
    "- DeletionVectors = disabled\n",
    "- ChangeDataFeed = enabled "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c8fe2b3e-49b9-41b4-88fb-a7da1e3757ea",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "ALTER TABLE delay_prediction_shap SET TBLPROPERTIES (\n",
    "  delta.enableChangeDataFeed = true,\n",
    "  delta.enableDeletionVectors = false\n",
    ");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6967d68b-d57d-4e63-89ea-4dfcf9eb2494",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "3"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 7865554604181675,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "03_Payment_Delay_Inference",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
