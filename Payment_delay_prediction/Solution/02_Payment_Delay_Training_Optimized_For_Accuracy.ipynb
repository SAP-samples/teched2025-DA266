{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1ce2dad4-e806-4270-a0c5-79dfeec2152b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "## Payment Delay Training \n",
    "\n",
    "This notebook is designed to train a model for predicting payment delays. We will load and prepare the preprocessed data, handle missing values and encode categorical variables. The model training then consists of several steps to fit the models and select the best parameters for our final model. We will evaluate the models using various metrics to determine the best model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6d658da2-cbaa-4b2e-ae82-ffd5698193d9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Install and Import Packages\n",
    "All necessary packages for this notebook are going to be outlined in the following notebook cell. In order to make sure that the results are reproducible, the following packages are going to be installed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2899d8a3-c072-4e73-bf7f-bcf4b42dfef0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%pip install xgboost\n",
    "%pip install optuna\n",
    "%pip install optuna-dashboard\n",
    "%restart_python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3e0d7806-b131-4ab9-9947-c308fa0a15c0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import mlflow\n",
    "from mlflow.models import infer_signature\n",
    "from mlflow.client import MlflowClient\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_percentage_error\n",
    "import optuna\n",
    "import optuna.visualization as ov\n",
    "from sklearn.model_selection import train_test_split\n",
    "from pathlib import Path\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a40e7de8-c109-47ff-b1ed-109f326d9348",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Setup Spark Session and consume data product\n",
    "In order to isolate the created data assets, we create a catalog within Databricks and a respective schema within the catalog. Please replace the values `<CATALOG_NAME>` and `<SCHEMA_NAME>` with the specific values that match our use case and group. You can find the correct names by checking the **Unity Catalog** and look for the specific catalog and schema names:`uc_XXX`, `grpX`.\n",
    "\n",
    "Please note: \n",
    "We adapted the code here to match our use case. Therefore, some of the lines are commented out and not needed. However, they can be useful for future applications. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e5b2269f-5d8e-41d6-8d77-4186955420d8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "-- CREATE CATALOG IF NOT EXISTS <CATALOG_NAME>;\n",
    "SET CATALOG uc_delayed_payments;\n",
    "-- CREATE SCHEMA IF NOT EXISTS <SCHEMA_NAME>;\n",
    "USE SCHEMA grp01;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "979690d8-79ef-4edb-9ced-6efb6fe25fe5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Prepare Data\n",
    "Replace the value `<TABLE_NAME>` with the name of the table that we created with the data preparation notebook. Additionally, set the value `<SEED_PARAMETER>` with a random number. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f8a10ca1-1a16-4382-a73c-6b11b6f29ce7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "data = (spark.read.table(\"prepared_accounting_document\").\n",
    "    where(col(\"delay\").isNotNull()).\n",
    "    drop(\"ClearingDate\", \"NetDueDate\").\n",
    "    sample(0.25, seed=42))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "366e208b-1531-4194-8232-4f06c9c0eacc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Create Training Data\n",
    "In the following step we will create a train-test split and focus on the column `delay`. We will also check the data types and adjust accordingly. Once the data is ready, we can continue with the actual model training\n",
    "\n",
    "Please adjust the code by setting the values for `<SPLIT>`. We aim for an 80% / 20% train-test split. Also, set a random number for the value `<SEED_PARAMETER>`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "daa7b984-2253-4c78-9aad-ad0825370ac1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "mlflow.set_tracking_uri(\"databricks\")\n",
    "mlflow.set_registry_uri(\"databricks-uc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "89655b50-23d8-413c-ae19-b096337159e1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "train_df, test_df = data.drop(\"CompanyCode\", \"AccountingDocument\", \"FiscalYear\", \"AccountingDocumentItem\").randomSplit([0.8, 0.2], seed=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d01b488d-e2e9-490e-ab80-7c338997062b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "The next cell converts the Spark DataFrames `train_df` and `test_df` into Pandas DataFrames.\n",
    "It separates the target variable \"delay\" from the features for both training and testing datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a1ccf659-b879-4063-89fc-def603c41ddf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "train_target = train_df.select(\"delay\").toPandas()\n",
    "train_data = train_df.drop(\"delay\").toPandas()\n",
    "test_target = test_df.select(\"delay\").toPandas()\n",
    "test_data = test_df.drop(\"delay\").toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1152820a-71c8-4e8a-9bf5-f215c5a691a9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "In the following function `infer_column_dtype` we want to return the strings `numeric`, `datetime` or `boolean` depending on the data type of the column. Please adjust the return statements accordingly by replacing `<DTYPE>`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f93867ec-306f-4e9b-8ec8-9a7f185ea39d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def infer_column_dtype(series):\n",
    "    # Try to convert to numeric\n",
    "    try:\n",
    "        pd.to_numeric(series.dropna())\n",
    "        return 'numeric'\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    # Try to convert to datetime\n",
    "    try:\n",
    "        pd.to_datetime(series.dropna(), errors='raise', infer_datetime_format=True)\n",
    "        return 'datetime'\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    # If all unique values are 'True' or 'False' like\n",
    "    lower_vals = set(str(v).strip().lower() for v in series.dropna().unique())\n",
    "    if lower_vals <= {'true', 'false', '1', '0'}:\n",
    "        return 'boolean'\n",
    "    \n",
    "    return 'string'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bdf2ba4d-5b9a-4d0a-9bdd-63fc2fc0aa71",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# apply to train data\n",
    "for col in train_data.columns:\n",
    "    inferred = infer_column_dtype(train_data[col])\n",
    "    if inferred == 'numeric':\n",
    "        train_data[col] = pd.to_numeric(train_data[col], errors='coerce')\n",
    "    elif inferred == 'datetime':\n",
    "        train_data[col] = pd.to_datetime(train_data[col], errors='coerce')\n",
    "    elif inferred == 'boolean':\n",
    "        train_data[col] = train_data[col].astype('bool')\n",
    "    else:\n",
    "        train_data[col] = train_data[col].astype('category')\n",
    "\n",
    "# apply to test data\n",
    "for col in test_data.columns:\n",
    "    inferred = infer_column_dtype(test_data[col])\n",
    "    if inferred == 'numeric':\n",
    "        test_data[col] = pd.to_numeric(test_data[col], errors='coerce')\n",
    "    elif inferred == 'datetime':\n",
    "        test_data[col] = pd.to_datetime(test_data[col], errors='coerce')\n",
    "    elif inferred == 'boolean':\n",
    "        test_data[col] = test_data[col].astype('bool')\n",
    "    else:\n",
    "        test_data[col] = test_data[col].astype('category')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ff906dec-2f23-4cb9-8e63-d99ecf391d49",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Model Training\n",
    "The function `model_training` performs the following steps:\n",
    "\n",
    "1. Load the training data\n",
    "2. Set model parameters\n",
    "3. Create train-test split\n",
    "4. Train the model\n",
    "5. Evaluate the model's performance on the validation dataset\n",
    "6. Fine-tune the model parameters for optimal performance\n",
    "\n",
    "Running the code may take a lot of time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4a694528-29d0-4ecb-a32c-fc202bce9fab",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "mlflow.xgboost.autolog(log_input_examples=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "73528a07-34f6-412f-b50b-ce9c2a357e89",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Using the prepared dataset, we perform hyperparameter tuning of an XGBoost regressor using the Optuna TPESampler. For our hyperparameter set we configure the parameters in `params`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ab0a57bf-c001-4de8-b61c-00b289d5b6a4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def model_training(trial: optuna.trial.Trial, X: pd.DataFrame, y: pd.Series):\n",
    "    # Convert unsupported data types\n",
    "    X = X.copy()\n",
    "    for col in X.select_dtypes(include=['category', 'datetime64[ns, UTC]']).columns:\n",
    "        if X[col].dtype.name == 'category':\n",
    "            X[col] = X[col].astype('category').cat.codes\n",
    "        elif X[col].dtype.name == 'datetime64[ns, UTC]':\n",
    "            X[col] = X[col].astype('int64')  \n",
    "\n",
    "    params = {\n",
    "        \"booster\": \"gbtree\",\n",
    "        \"objective\": \"reg:squarederror\",\n",
    "        \"tree_method\": \"hist\",\n",
    "        \"max_depth\": trial.suggest_int(\"max_depth\", 3, 10),\n",
    "        \"learning_rate\": trial.suggest_float(\"learning_rate\", 1e-2, 3e-1, log=True),\n",
    "        \"n_estimators\": trial.suggest_int(\"n_estimators\", 200, 1500),\n",
    "        \"subsample\": trial.suggest_float(\"subsample\", 0.5, 1.0),\n",
    "        \"colsample_bytree\": trial.suggest_float(\"colsample_bytree\", 0.5, 1.0),\n",
    "        \"gamma\": trial.suggest_float(\"gamma\", 0.0, 5.0),\n",
    "        \"min_child_weight\": trial.suggest_float(\"min_child_weight\", 1.0, 10.0),\n",
    "        \"reg_alpha\": trial.suggest_float(\"reg_alpha\", 0.0, 1.0),\n",
    "        \"reg_lambda\": trial.suggest_float(\"reg_lambda\", 0.0, 1.0),\n",
    "        \"random_state\": 42,\n",
    "    }\n",
    "\n",
    "    X_train, X_valid, y_train, y_valid = train_test_split(\n",
    "        X, y, test_size=0.2, random_state=42\n",
    "    )\n",
    "\n",
    "    model = XGBRegressor(**params, enable_categorical=True, n_jobs=-1, early_stopping_rounds=10)\n",
    "    model.fit(\n",
    "        X_train,\n",
    "        y_train,\n",
    "        eval_set=[(X_valid, y_valid)],\n",
    "        verbose=False\n",
    "    )\n",
    "    preds = model.predict(X_valid)\n",
    "    mse_score = mean_squared_error(y_valid, preds)\n",
    "    mape_score = mean_absolute_percentage_error(y_valid, preds)\n",
    "    r2_metric = r2_score(y_valid, preds)\n",
    "    return mse_score\n",
    "\n",
    "mlflow.set_registry_uri(\"databricks-uc\")\n",
    "\n",
    "study = optuna.create_study(\n",
    "    direction=\"minimize\",\n",
    "    sampler=optuna.samplers.TPESampler(seed=42),\n",
    "    pruner=optuna.pruners.MedianPruner(n_warmup_steps=5),\n",
    ")\n",
    "study.optimize(lambda trial: model_training(trial, train_data, train_target), n_trials=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0899c774-60ab-4831-8ab8-c23e7b5890b0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Show Plots\n",
    "Next we will have a look at the optimization plots and check which of the parameters work best for our use case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e0c93633-3c0a-4a53-bdfd-48756b06e2da",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8f26b158-bc62-4530-805c-8422b11210c4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "ov.plot_optimization_history(study)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9e02b66d-b7cd-489a-b585-b6587cf5dc5b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "ov.plot_param_importances(study)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2a8ae1df-246c-401d-8454-cca808248cd9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "ov.plot_parallel_coordinate(study)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "75b9fcb9-d4e6-4b5e-a8f5-9e7b7fede259",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Retrieve Optimal Parameters\n",
    "To retrieve the best parameters for our final model you can run the following code and check the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "490f8e3a-184f-47a7-882c-77c3de5de259",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(\"Number of finished trials:\", len(study.trials))\n",
    "print(\"Best MSE:\", study.best_value)\n",
    "print(\"Best params:\")\n",
    "for k, v in study.best_params.items():\n",
    "    print(f\"  {k}: {v}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8bb64cba-1ee3-4424-af0d-1a7f21b564a6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Fit Model\n",
    "Now we will fit the model using the preferred parameters and finally register the model. We will set the alias `prod` for the registered model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "81ca4698-28ed-48f0-ae5b-b61201dd81e5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Convert datetime column to numerical format\n",
    "# train_data['__TIMESTAMP'] = train_data['__TIMESTAMP'].astype('int64')\n",
    "# test_data['__TIMESTAMP'] = test_data['__TIMESTAMP'].astype('int64')\n",
    "\n",
    "# Ensure categorical columns are properly encoded\n",
    "categorical_columns = train_data.select_dtypes(include=['category']).columns\n",
    "train_data[categorical_columns] = train_data[categorical_columns].apply(lambda x: x.cat.codes)\n",
    "test_data[categorical_columns] = test_data[categorical_columns].apply(lambda x: x.cat.codes)\n",
    "\n",
    "with mlflow.start_run(run_name=\"Delay_Prediction_Training\") as run:\n",
    "    final_xgbmodel = XGBRegressor(\n",
    "        **study.best_params,\n",
    "        enable_categorical=True,\n",
    "        n_jobs=-1\n",
    "    )\n",
    "    final_xgbmodel.fit(\n",
    "        train_data,\n",
    "        train_target,\n",
    "        verbose=False\n",
    "    )\n",
    "    predictions = final_xgbmodel.predict(test_data)\n",
    "    mse = mean_squared_error(test_target, predictions)\n",
    "    r2_metric = r2_score(test_target, predictions)\n",
    "    mape_score = mean_absolute_percentage_error(test_target, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5ac05c0f-b288-437c-add0-4a6a66b3a1f0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "trained_model = mlflow.register_model(f\"runs:/{run.info.run_id}/model\", \"delay_prediction\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "042b4bb3-8042-4e11-85be-71177299925d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Please set the alias for the trained model with `prod` by replacing the value `<ALIAS>` accordingly. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9e1778fe-b2ba-4190-874b-797074fe72e5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "mlflow_client = mlflow.MlflowClient()\n",
    "mlflow_client.set_registered_model_alias(name=trained_model.name, alias=\"prod\", version=trained_model.version)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": {
    "hardware": {
     "accelerator": null,
     "gpuPoolId": null,
     "memory": "HIGH"
    }
   },
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 7147852492886113,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "02_Payment_Delay_Training_Optimized_For_Accuracy",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
