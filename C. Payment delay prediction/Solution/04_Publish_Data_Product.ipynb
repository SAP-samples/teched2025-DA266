{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "00ba7deb-1094-4a9a-84cf-b2f11363566a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Publish a Data Product\n",
    "There are several steps to publish a data product to BDC:\n",
    "\n",
    "1. Install SAP SDK\n",
    "2. Create secret scope \n",
    "3. Create client\n",
    "4. Create share\n",
    "5. Add recipient to share\n",
    "6. Create ORD object\n",
    "7. Create CSN object\n",
    "8. Publish data product\n",
    "\n",
    "\n",
    "### 1. Install and Load Packages\n",
    "To be able to share the enhanced data products back to SAP Business Data Cloud, we need the `SAP SDK`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b41eb27c-0f38-4869-9c77-52d1dcd3fc60",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting sap-bdc-connect-sdk\n  Downloading sap_bdc_connect_sdk-1.0.10-py3-none-any.whl.metadata (8.4 kB)\nRequirement already satisfied: typing-extensions<=4.12.0 in /databricks/python3/lib/python3.11/site-packages (from sap-bdc-connect-sdk) (4.10.0)\nRequirement already satisfied: urllib3<2.1 in /databricks/python3/lib/python3.11/site-packages (from sap-bdc-connect-sdk) (1.26.16)\nRequirement already satisfied: pydantic<=2.10.6 in /databricks/python3/lib/python3.11/site-packages (from sap-bdc-connect-sdk) (1.10.6)\nCollecting argparse~=1.4.0 (from sap-bdc-connect-sdk)\n  Downloading argparse-1.4.0-py2.py3-none-any.whl.metadata (2.8 kB)\nRequirement already satisfied: python-dateutil<=2.9.0 in /databricks/python3/lib/python3.11/site-packages (from sap-bdc-connect-sdk) (2.8.2)\nRequirement already satisfied: requests<=2.32.0 in /databricks/python3/lib/python3.11/site-packages (from sap-bdc-connect-sdk) (2.31.0)\nRequirement already satisfied: cryptography>=41.0.3 in /databricks/python3/lib/python3.11/site-packages (from sap-bdc-connect-sdk) (41.0.3)\nRequirement already satisfied: protobuf<=5.29.3 in /databricks/python3/lib/python3.11/site-packages (from sap-bdc-connect-sdk) (5.29.3)\nRequirement already satisfied: grpcio-status>=1.62.3 in /databricks/python3/lib/python3.11/site-packages (from sap-bdc-connect-sdk) (1.69.0)\nRequirement already satisfied: cffi>=1.12 in /databricks/python3/lib/python3.11/site-packages (from cryptography>=41.0.3->sap-bdc-connect-sdk) (1.15.1)\nRequirement already satisfied: grpcio>=1.69.0 in /databricks/python3/lib/python3.11/site-packages (from grpcio-status>=1.62.3->sap-bdc-connect-sdk) (1.69.0)\nRequirement already satisfied: googleapis-common-protos>=1.5.5 in /databricks/python3/lib/python3.11/site-packages (from grpcio-status>=1.62.3->sap-bdc-connect-sdk) (1.65.0)\nRequirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from python-dateutil<=2.9.0->sap-bdc-connect-sdk) (1.16.0)\nRequirement already satisfied: charset-normalizer<4,>=2 in /databricks/python3/lib/python3.11/site-packages (from requests<=2.32.0->sap-bdc-connect-sdk) (2.0.4)\nRequirement already satisfied: idna<4,>=2.5 in /databricks/python3/lib/python3.11/site-packages (from requests<=2.32.0->sap-bdc-connect-sdk) (3.4)\nRequirement already satisfied: certifi>=2017.4.17 in /databricks/python3/lib/python3.11/site-packages (from requests<=2.32.0->sap-bdc-connect-sdk) (2023.7.22)\nRequirement already satisfied: pycparser in /databricks/python3/lib/python3.11/site-packages (from cffi>=1.12->cryptography>=41.0.3->sap-bdc-connect-sdk) (2.21)\nDownloading sap_bdc_connect_sdk-1.0.10-py3-none-any.whl (43 kB)\nDownloading argparse-1.4.0-py2.py3-none-any.whl (23 kB)\nInstalling collected packages: argparse, sap-bdc-connect-sdk\nSuccessfully installed argparse-1.4.0 sap-bdc-connect-sdk-1.0.10\n\u001B[43mNote: you may need to restart the kernel using %restart_python or dbutils.library.restartPython() to use updated packages.\u001B[0m\nRequirement already satisfied: pydantic in /databricks/python3/lib/python3.11/site-packages (1.10.6)\nCollecting pydantic\n  Downloading pydantic-2.11.7-py3-none-any.whl.metadata (67 kB)\nCollecting annotated-types>=0.6.0 (from pydantic)\n  Downloading annotated_types-0.7.0-py3-none-any.whl.metadata (15 kB)\nCollecting pydantic-core==2.33.2 (from pydantic)\n  Downloading pydantic_core-2.33.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.8 kB)\nCollecting typing-extensions>=4.12.2 (from pydantic)\n  Downloading typing_extensions-4.14.1-py3-none-any.whl.metadata (3.0 kB)\nCollecting typing-inspection>=0.4.0 (from pydantic)\n  Downloading typing_inspection-0.4.1-py3-none-any.whl.metadata (2.6 kB)\nDownloading pydantic-2.11.7-py3-none-any.whl (444 kB)\nDownloading pydantic_core-2.33.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.0 MB)\n\u001B[?25l   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m0.0/2.0 MB\u001B[0m \u001B[31m?\u001B[0m eta \u001B[36m-:--:--\u001B[0m\n\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m2.0/2.0 MB\u001B[0m \u001B[31m61.1 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n\u001B[?25hDownloading annotated_types-0.7.0-py3-none-any.whl (13 kB)\nDownloading typing_extensions-4.14.1-py3-none-any.whl (43 kB)\nDownloading typing_inspection-0.4.1-py3-none-any.whl (14 kB)\nInstalling collected packages: typing-extensions, annotated-types, typing-inspection, pydantic-core, pydantic\n  Attempting uninstall: typing-extensions\n    Found existing installation: typing_extensions 4.10.0\n    Not uninstalling typing-extensions at /databricks/python3/lib/python3.11/site-packages, outside environment /local_disk0/.ephemeral_nfs/envs/pythonEnv-029af1c7-cb89-401e-899d-da58509b698c\n    Can't uninstall 'typing_extensions'. No files were found to uninstall.\n  Attempting uninstall: pydantic\n    Found existing installation: pydantic 1.10.6\n    Not uninstalling pydantic at /databricks/python3/lib/python3.11/site-packages, outside environment /local_disk0/.ephemeral_nfs/envs/pythonEnv-029af1c7-cb89-401e-899d-da58509b698c\n    Can't uninstall 'pydantic'. No files were found to uninstall.\n\u001B[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nsap-bdc-connect-sdk 1.0.10 requires pydantic<=2.10.6, but you have pydantic 2.11.7 which is incompatible.\nsap-bdc-connect-sdk 1.0.10 requires typing-extensions<=4.12.0, but you have typing-extensions 4.14.1 which is incompatible.\u001B[0m\u001B[31m\n\u001B[0mSuccessfully installed annotated-types-0.7.0 pydantic-2.11.7 pydantic-core-2.33.2 typing-extensions-4.14.1 typing-inspection-0.4.1\n\u001B[43mNote: you may need to restart the kernel using %restart_python or dbutils.library.restartPython() to use updated packages.\u001B[0m\n"
     ]
    }
   ],
   "source": [
    "%pip install sap-bdc-connect-sdk\n",
    "%pip install --upgrade pydantic\n",
    "%restart_python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "02fe2b63-69d4-46ff-b3dc-6d1c6520bf11",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: sap-bdc-connect-sdk\nVersion: 1.0.10\nSummary: Python SDK for SAP Business Data Cloud\nHome-page: https://www.sap.com\nAuthor: SAP SE\nAuthor-email: \nLicense: SAP DEVELOPER LICENSE AGREEMENT\nLocation: /local_disk0/.ephemeral_nfs/envs/pythonEnv-029af1c7-cb89-401e-899d-da58509b698c/lib/python3.11/site-packages\nRequires: argparse, cryptography, grpcio-status, protobuf, pydantic, python-dateutil, requests, typing-extensions, urllib3\nRequired-by: \n"
     ]
    }
   ],
   "source": [
    "pip show sap-bdc-connect-sdk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3b353142-d9c7-476a-96f4-a4f9d2c63a6b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 2. Create secret scope\n",
    "\n",
    "This step has already been done by the administrator to facilitate the sharing process. However, if you need to do it on you own you can follow the steps described here to create a secret scope:\n",
    "\n",
    "To create a secret scope you can either use the following URL https://<databricks-instance>#secrets/createScope. Replace <databricks-instance> with the workspace URL of your Databricks deployment.\n",
    "\n",
    "Alternatively, you can run the following command in the terminal by clicking on the terminal icon on the lower right corner: `databricks secrets create-scope sap-bdc-connect-sdk`. \n",
    "\n",
    "The secret scope only has to be created once and can be made accessible to all workspace users by either toggling `manage principal` to `all workspace users` or via the terminal using the following command `databricks secrets put-acl sap-bdc-connect-sdk users READ`. To check whether the assignment worked, you can then use the command `databricks secrets list-acls sap-bdc-connect-sdk`.\n",
    "\n",
    "A full explanation can be found here https://docs.databricks.com/aws/en/security/secrets/example-secret-workflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "83fa375d-1243-4d2d-a462-9b116f6eb2e9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 3. Create a Client\n",
    "The `DatabricksClient` receives dbutils as a parameter, which is a Databricks utility that can be used inside the Databricks notebooks.\n",
    "\n",
    "The `BdcConnectClient` receives the DatabricksClient as a parameter to get information from the Databricks environment (e.g. secrets, api_token, workspace_url_base)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ccc8a15a-ea70-44ec-8785-588203dbe280",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from bdc_connect_sdk.auth import BdcConnectClient\n",
    "from bdc_connect_sdk.auth import DatabricksClient\n",
    "from bdc_connect_sdk.utils import csn_generator\n",
    "import json\n",
    "\n",
    "databricks_client = DatabricksClient(dbutils)\n",
    "bdc_connect_client = BdcConnectClient(databricks_client)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1aaff699-6404-4d16-aa60-f867af2ab311",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 4. Create or Update Share\n",
    "A share is a mechanism for distributing and accessing data across different systems. Creating or updating a share involves including specific attributes, such as @openResourceDiscoveryV1, in the request body, aligning with the Open Resource Discovery protocol. This procedure ensures that the share is properly structured and described according to specified standards, facilitating effective data sharing and management. This is done in step 5.\n",
    "\n",
    "In the following code you'll need to set the parameters for the share:\n",
    "- `<SHARE_NAME>`\n",
    "- `<SHORT_DESCRIPTION>`\n",
    "- `<LONG_DESCRIPTION>`\n",
    "- `<DATA_PRODUCT_NAME>`\n",
    "\n",
    "To be able to share a data product, the following requirements should be fulfilled:\n",
    "- table has primary keys \n",
    "- DeletionVectors = disabled\n",
    "- ChangeDataFeed = enabled "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "96dbe1f0-ddf5-46bd-a619-66da4052e9f2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "We again set the location to the according catalog and schema."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b4442587-e6c9-49fc-b44e-ef9eacc1e6f4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "-- CREATE CATALOG IF NOT EXISTS <CATALOG_NAME>;\n",
    "SET CATALOG uc_delayed_payment;\n",
    "-- CREATE SCHEMA IF NOT EXISTS <SCHEMA_NAME>;\n",
    "USE SCHEMA grp1;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "510ea854-a9ab-499d-a1e5-7ea8e5decebe",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "CREATE SHARE IF NOT EXISTS delayed_payment_grp1;\n",
    "ALTER SHARE delayed_payment_grp1 ADD TABLE delay_prediction_dataset_shap WITH HISTORY;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8d3950e1-ae54-435e-b78f-2f7aeb2bae22",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 5. Add Recipient to Share\n",
    "\n",
    "To be able to share the data product to BDC we need to select `sap-business-data-cloud` as the recipient for the share we've just created. This can be either done by heading over to the **Unity Catalog** > **Delta Sharing** > **Shared by me** > *Select Share* > **Recipient** > **Add Recipients** > *Select sap-business-data-cloud*\n",
    "\n",
    "Alternatively, the recipient can be added using the following query. Please fill in the `<SHARE_NAME>` and the `<RECIPIENT_NAME>`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1a08c29a-869e-4839-882b-9fc9b7707196",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "GRANT SELECT ON SHARE delayed_payment_grp1 TO RECIPIENT `sap-business-data-cloud`;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3d9bb856-c490-4a80-97c8-a00f53fce45c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 6. Create ORD object \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6e0e01ea-cd9f-4b86-b2ff-afbeaf4b4c7d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "with BdcConnectClient(DatabricksClient(dbutils)) as bdc_connect_client:\n",
    "    share_name = \"delayed_payment_grp1\"\n",
    "\n",
    "try:\n",
    "    share_body = {\n",
    "        \"type\": \"REMOTE_SHARE\",\n",
    "        \"provider\": {\n",
    "            \"type\": \"FEDERATION\",\n",
    "            \"name\": \"databricks\"\n",
    "        },\n",
    "        \"@openResourceDiscoveryV1\": {\n",
    "            \"title\": \"Delayed Payment Prediction Data Product\",\n",
    "            \"shortDescription\": \"Data asset for payment predictions\",\n",
    "            \"description\": \"This data product contains the data used for the payment prediction model.\"\n",
    "        }\n",
    "    }\n",
    "\n",
    "    share_request_body = json.dumps(share_body)\n",
    "\n",
    "    catalog_response = bdc_connect_client.create_or_update_share(\n",
    "        share_name,\n",
    "        share_request_body\n",
    "    )\n",
    "except Exception as ex:\n",
    "    print(f\"Exception when creating or updating share(name={share_name}): {ex}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3d92a3f5-9edc-462b-a2ea-c47105fe6499",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 7. Create CSN object\n",
    "\n",
    "The CSN serves as a standardized format for configuring and describing shares within a network. To create or update the CSN for a share, it's advised to prepare the CSN content in a separate file and include this content in the request body. This approach ensures accuracy and compliance with the CSN interoperability specifications, facilitating consistent and effective share configuration across systems.\n",
    "\n",
    "In the following code you'll need to set the parameter `<SHARE_NAME>` for the share."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7cea417d-e669-419f-85c6-a44dcc272656",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "with BdcConnectClient(DatabricksClient(dbutils)) as bdc_connect_client:\n",
    "    share_name = \"delayed_payment_grp1\"\n",
    "\n",
    "try:\n",
    "    csn_schema = csn_generator.generate_csn_template(share_name)\n",
    "    csn_schema_string = json.dumps(csn_schema)\n",
    "    \n",
    "    csn_response = bdc_connect_client.create_or_update_share_csn(\n",
    "        share_name,\n",
    "        csn_schema_string\n",
    "    )\n",
    "except Exception as ex:\n",
    "    print(f\"Exception when creating or updating CSN for share(name={share_name}): {ex}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c90796fd-53d6-4495-a662-f39ca3b0be95",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 8. Publish a Data Product\n",
    "A Data Product is an abstraction that represents a type of data or data set within a system, facilitating easier management and sharing across different platforms. It bundles resources or API endpoints to enable efficient data access and utilization by integrated systems. Publishing a Data Product allows these systems to access and consume the data, ensuring seamless communication and resource sharing.\n",
    "\n",
    "In the following code you'll need to set the parameter `<SHARE_NAME>` for the share."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "045aff67-f2d3-4f9c-9ca5-bd9ef83884b7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "with BdcConnectClient(DatabricksClient(dbutils)) as bdc_connect_client:\n",
    "    share_name = \"delayed_payment_grp1\"\n",
    "\n",
    "try:\n",
    "    publish_response = bdc_connect_client.publish_data_product(\n",
    "        share_name,\n",
    "    )\n",
    "except Exception as ex:\n",
    "    print(f\"Exception when publishing data product for share(name={share_name}): {ex}\\n\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 4788921342253638,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "04_Publish_Data_Product",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}